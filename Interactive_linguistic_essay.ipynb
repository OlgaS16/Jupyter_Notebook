{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBWtPGW3Yiour6TkXoqLe2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlgaS16/Jupyter_Notebook/blob/main/Interactive_linguistic_essay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I scrape the Wikipedia page about **Computer Security** and perform a basic linguistic analysis using Python libraries such as *BeautifulSoup*, *NLTK*, and *matplotlib*. The idea behind this project is to explore how language works in a formal, technical context by looking at the vocabulary, grammar, and structure used in online cybersecurity-related content.\n",
        "\n",
        "To do this, I first collect real-world data through web scraping, then preprocess and tokenize the text using natural language processing tools. After that, I analyze word frequency and grammatical categories to understand how information is structured in this type of writing. I also visualize language patterns using charts and graphs to make the findings clearer and more engaging.\n",
        "\n",
        "By the end, I hope to get a clearer picture of the kinds of words and sentence structures that dominate cybersecurity-related writing."
      ],
      "metadata": {
        "id": "ghWKJBNhnEAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(url=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Hacker_behind_PC.svg/800px-Hacker_behind_PC.svg.png\", width=400))"
      ],
      "metadata": {
        "id": "ptUMxd9qvwS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I use *requests* and *BeautifulSoup* to grab the main content from the Wikipedia page on **Computer Security**. I extract all the `<p>` tags, which contain the actual article paragraphs."
      ],
      "metadata": {
        "id": "5dAMqIAky0lZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/Computer_security\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "paragraphs = soup.find_all('p')\n",
        "text = ' '.join([p.get_text() for p in paragraphs])\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "VxDU0MOOwfrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I clean the scraped text by:\n",
        "- Tokenizing it\n",
        "- Lowercasing everything\n",
        "- Removing punctuation and common stopwords\n",
        "\n",
        "This helps us focus on the important words that actually tell us something about the topic."
      ],
      "metadata": {
        "id": "LiC1Me0xzNUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "filtered_words = [w for w in words if w not in stopwords.words('english')]\n",
        "\n",
        "print(filtered_words[:20])\n"
      ],
      "metadata": {
        "id": "B6iEcZP-wfk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I use NLTK’s *FreqDist* to find the most frequent content words in the article. These are likely to be technical terms related to cybersecurity, which shows how formal writing leans on domain-specific language.\n",
        "\n",
        "We also plot the top 30 with *matplotlib*."
      ],
      "metadata": {
        "id": "zberB7C3zdAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fdist = FreqDist(filtered_words)\n",
        "fdist.plot(30, title=\"Top 30 Most Frequent Words in 'Computer Security'\")\n"
      ],
      "metadata": {
        "id": "jbfykXU-w_o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an interactive chart that allows the reader to adjust the number of most frequent words displayed. By using the slider, one can explore how word frequency distribution shifts depending on how many top terms are visualized."
      ],
      "metadata": {
        "id": "hY2Jd-f70UiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def show_top_words(n):\n",
        "    fdist.plot(n, title=f\"Top {n} Most Frequent Words in 'Computer Security'\")\n",
        "\n",
        "widgets.interact(show_top_words, n=widgets.IntSlider(min=5, max=100, step=5, value=45));"
      ],
      "metadata": {
        "id": "7fVb-zgJ0Txj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I use *nltk.pos_tag()* to tag each word with its part of speech. This lets me see whether the article relies more on nouns (things), verbs (actions), adjectives (descriptions), etc.\n",
        "\n",
        "I then count and visualize the most common POS tags."
      ],
      "metadata": {
        "id": "JPO_uf7Dzp2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tagged = nltk.pos_tag(filtered_words)\n",
        "tagged[:10]\n"
      ],
      "metadata": {
        "id": "XwKpVV1ywwmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "pos_counts = Counter(tag for word, tag in tagged)\n",
        "common_tags = pos_counts.most_common(5)\n",
        "\n",
        "# Convert to a plot-friendly format\n",
        "labels, counts = zip(*common_tags)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title(\"Most Common POS Tags\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nzC-m-_hwyKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the article containts a lot of nouns, like **“attack”, “security”, “system”** — these carry most of the meaning. It also uses a fair amount of adjectives (like **“unauthorized”** or **“malicious”**) to describe threats and protections. Verbs are present but less dominant — technical writing tends to be more descriptive than action-based."
      ],
      "metadata": {
        "id": "mji2Wk1Iz09o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project helped me better understand the way technical language is used in written texts. I noticed that Wikipedia articles like this one rely heavily on **nouns** and **technical terms**, which makes sense since the goal is to inform and explain rather than entertain. Once the text was cleaned and processed, it became much easier to see patterns and identify the core vocabulary used throughout the article. Working with tools like *BeautifulSoup, NLTK*, and *matplotlib* showed me how well they complement each other—together, they made it possible to turn messy web data into something structured and visual. I also learned how to use *ipywidgets* to add interactive elements to the notebook, which made the whole analysis more dynamic and engaging."
      ],
      "metadata": {
        "id": "niWY7_hd1DLg"
      }
    }
  ]
}